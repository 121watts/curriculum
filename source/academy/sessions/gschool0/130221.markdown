---
layout: page
title: Thursday, February 21st
sidebar: true
---

## SalesEngine Submission Notes

### Submission URL

ASAP you need to edit and submit a pull request with your project URL here:

https://github.com/gSchool/submissions/blob/master/projects/sales_engine.markdown

### Test Coverage

Your test suite should be generating a report in `coverage/index.html` with your test coverage percentage every time the suite is run.

When setup SimpleCov in your `test_helper.rb` it probably looks like this:

```ruby
$:.unshift("lib")

require 'simplecov'
SimpleCov.start do
  add_filter "/test/"
end
```

### Code Cleanliness

Make sure you've added the cane and reek rake tasks as posted yesterday:

{% terminal %}
$ git remote add upstream git://github.com/gSchool/sales_engine.git
$ git pull upstream master
{% endterminal %}

You'll likely get a merge conflict in both your `Gemfile` and your `Rakefile`. You'll need to manually clean those up in your editor, let us know if you need help. Once those conflicts are resolved:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:lines
$ bundle exec rake sanitation:methods
{% endterminal %}

Any reported issues will affect your style scoring -- fix them up if you have time!

## Evaluating SalesEngine

You will peer assess *three different SalesEngine projects* using the instructions below.

### Evaluation Assignments

* Daniel Mee & Raphael Weiner evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Kyle Suss & John Maddux
  * Laura Steadman & Elaine Tai
* Kyle Suss & John Maddux evaluate projects by:
  * Bradley Sheehan & Kareem Grant
  * Daniel Mee & Raphael Weiner
  * Laura Steadman & Elaine Tai
* Laura Steadman & Elaine Tai evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Aimee Maher & Blair Anderson
  * Kyle Suss & John Maddux
* Bradley Sheehan & Kareem Grant evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Jorge Tellez & Ron Rateau
  * Jennifer Eliuk & Josh Mejia
* Aimee Maher & Blair Anderson evaluate projects by:
  * Bradley Sheehan & Kareem Grant
  * Christopher Knight & Danny Garcia
  * Paul Blackwell & James Denman
* Paul Blackwell & James Denman evaluate projects by:
  * Daniel Mee & Raphael Weiner
  * Jorge Tellez & Ron Rateau
  * Jennifer Eliuk & Josh Mejia
* Jennifer Eliuk & Josh Mejia evaluate projects by:
  * Christopher Knight & Danny Garcia
  * Paul Blackwell & James Denman
  * Daniel Mee & Raphael Weiner
* Phil Battos & Geoffrey Schorkopf evaluate projects by:
  * Christopher Knight & Danny Garcia
  * Jennifer Eliuk & Josh Mejia
  * Jorge Tellez & Ron Rateau
* Logan Sears & Chelsea Komlo evaluate projects by:
  * Shane Rogers & Erin Drummond
  * Kyle Suss & John Maddux
  * Paul Blackwell & James Denman
* Christopher Knight & Danny Garcia evaluate projects by:
  * Logan Sears & Chelsea Komlo
  * Bradley Sheehan & Kareem Grant
  * Shane Rogers & Erin Drummond
* Jorge Tellez & Ron Rateau evaluate projects by:
  * Logan Sears & Chelsea Komlo
  * Aimee Maher & Blair Anderson
  * Shane Rogers & Erin Drummond
* Shane Rogers & Erin Drummond evaluate projects by:
  * Aimee Maher & Blair Anderson
  * Logan Sears & Chelsea Komlo
  * Laura Steadman & Elaine Tai

### Starting the Eval

Open http://eval.jumpstartlab.com and begin the evaluation for your first peer pair.

### Setup Folders and Code

* Create a folder on your system named `sales_engine_evaluation_1`
* `cd` into that directory
* Clone the project you're evaluating by substituting the correct URL into this instruction:

{% terminal %}
git clone git://github.com/username/sales_engine.git
{% endterminal %}

* Clone the spec harness repository within the same `sales_engine_evaluation` folder:

{% terminal %}
git clone git://github.com/gSchool/sales_engine_spec_harness.git
{% endterminal %}

### Correctness - Setup & Run the Spec Harness

Go into the harness directory, install dependencies, and run the harness tests:

{% terminal %}
$ cd sales_engine_spec_harness
$ bundle
$ rake spec
{% endterminal %}

Then record your findings in the evaluation form.

```plain
Correctness
4: All provided tests pass without an error or crash
3: One test failed or crashed
2: Two or three tests failed or crashed
1: More than three tests failed or crashed
0: Program will not run
```

### Effort - Evaluating Extensions

There are individual rake tasks for each set of extensions. Run each of these to see if their code passes the extension tests:

{% terminal %}
$ rake spec:extensions:merchant
$ rake spec:extensions:invoice
$ rake spec:extensions:customer
{% endterminal %}

Use this data in combination with results from the base expectations tests (above) to determine and record the Effort score.

```plain
Effort
5: Program fulfills all Base Expectations and three Extensions
4: Program fulfills all Base Expectations and two Extensions
3: Program fulfills all Base Expectations
2: Program fulfills Base Expectations except for one or two features.
1: Program fulfills some Base Expectations, but more than two features are broken.
0: Program does not fulfill any of the Base Expectations
```

### Testing - Evaluate Test Coverage

Next you'll need to check out their test coverage from within their project:

{% terminal %}
$ cd ../sales_engine
$ bundle
$ rake test
$ open coverage/index.html
{% endterminal %}

Use the code coverage percentage to determine and record the score.

```plain
Testing
4: Testing suite covers >95% of application code
3: Testing suite covers 90-94% of application code
2: Testing suite covers 80-89% of application code
1: Testing suite covers 50-89% of application code
0: Testing suite covers <50% of application code
```

### Style - Evaluate Code Style

Use the automated rake tasks to evaluate the code style from within the project directory:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:all
{% endterminal %}

Count the number of style infractions and determine/record the score.

```plain
Style
4: Source code consistently uses strong code style including lines under 80 characters, methods under 10 lines of code, and correct indentation.
3: Source code uses good code style, but breaks the above criteria in three or fewer spots
2: Source code uses mixed style, with three to six style breaks
1: Source code is generally messy with six to twelve issues
0: Source code is unacceptable, containing more than twelve style issues
```

### Submit the Evaluation

Review all your scores for accuracy, add comments you have, and submit the eval.

### `3.times`

Go back and repeat this process for the other two pairs you're evaluating. It's recommended to start with a totally new directory to hold the second project and a second copy of the spec harness, just to make 100% sure you don't accidentially run the first pair's code during the second eval.
