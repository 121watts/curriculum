---
layout: page
title: Thursday, February 21st
sidebar: true
---

## Evaluating SalesEngine

You will peer assess *three different SalesEngine projects* using the instructions below.

### Evaluation Assignments

* Daniel Mee & Raphael Weiner evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Kyle Suss & John Maddux
  * Laura Steadman & Elaine Tai
* Kyle Suss & John Maddux evaluate projects by:
  * Bradley Sheehan & Kareem Grant
  * Daniel Mee & Raphael Weiner
  * Laura Steadman & Elaine Tai
* Laura Steadman & Elaine Tai evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Aimee Maher & Blair Anderson
  * Kyle Suss & John Maddux
* Bradley Sheehan & Kareem Grant evaluate projects by:
  * Phil Battos & Geoffrey Schorkopf
  * Jorge Tellez & Ron Rateau
  * Jennifer Eliuk & Josh Mejia
* Aimee Maher & Blair Anderson evaluate projects by:
  * Bradley Sheehan & Kareem Grant
  * Christopher Knight & Danny Garcia
  * Paul Blackwell & James Denman
* Paul Blackwell & James Denman evaluate projects by:
  * Daniel Mee & Raphael Weiner
  * Jorge Tellez & Ron Rateau
  * Jennifer Eliuk & Josh Mejia
* Jennifer Eliuk & Josh Mejia evaluate projects by:
  * Christopher Knight & Danny Garcia
  * Paul Blackwell & James Denman
  * Daniel Mee & Raphael Weiner
* Phil Battos & Geoffrey Schorkopf evaluate projects by:
  * Christopher Knight & Danny Garcia
  * Jennifer Eliuk & Josh Mejia
  * Jorge Tellez & Ron Rateau
* Logan Sears & Chelsea Komlo evaluate projects by:
  * Shane Rogers & Erin Drummond
  * Kyle Suss & John Maddux
  * Paul Blackwell & James Denman
* Christopher Knight & Danny Garcia evaluate projects by:
  * Logan Sears & Chelsea Komlo
  * Bradley Sheehan & Kareem Grant
  * Shane Rogers & Erin Drummond
* Jorge Tellez & Ron Rateau evaluate projects by:
  * Logan Sears & Chelsea Komlo
  * Aimee Maher & Blair Anderson
  * Shane Rogers & Erin Drummond
* Shane Rogers & Erin Drummond evaluate projects by:
  * Aimee Maher & Blair Anderson
  * Logan Sears & Chelsea Komlo
  * Laura Steadman & Elaine Tai

### Staring the Eval

Open http://eval.jumpstartlab.com and begin the evaluation for your first peer pair.

### Setup Folders and Code

* Create a folder on your system named `sales_engine_evaluation`
* `cd` into that directory
* Clone the project you're evaluating by substituting the correct URL into this instruction:

```
git clone git://github.com/username/sales_engine.git
```

* Clone the spec harness repository within the same `sales_engine_evaluation` folder:

```
git clone git://github.com/gSchool/sales_engine_spec_harness.git
```

### Setup & Run the Spec Harness

Go into the harness directory, install dependencies, and run the harness tests:

{% terminal %}
$ cd sales_engine_spec_harness
$ bundle
$ rake spec
{% endterminal %}

Then record your findings in the evaluation form.

### Evaluating Extensions

There are individual rake tasks for each set of extensions. Run each of these to see if their code passes the extension tests:

{% terminal %}
$ rake spec:extensions:merchant
$ rake spec:extensions:invoice
$ rake spec:extensions:customer
{% endterminal %}

Use this data in combination with results from the base expectations tests (above) to determine and record the Effort score.

### Evaluate Test Coverage

Next you'll need to check out their test coverage from within their project:

{% terminal %}
$ cd ../sales_engine
$ bundle
$ rake test
$ open coverage/index.html
{% endterminal %}

Use the code coverage percentage to determine and record the score.

### Evaluate Code Style

Use the automated tools `cane` and `reek` to evaluate the code style:

{% terminal %}
$ gem install cane reek
$ cane --style-glob 'lib/**/*.rb'
$ reek . 2>&1 | grep "(LongMethod)"
$ open coverage/index.html
{% endterminal %}

Count the number of style infractions and determine/record the score.

### Submit the Evaluation

Review all your scores for accuracy, add comments you have, and submit the eval.

### `2.times`

Go back and repeat this process for the other two pairs you're evaluating. It's recommended to start with a totally new directory to hold the second project and a second copy of the spec harness, just to make 100% sure you don't accidentially run the first pair's code during the second eval.
