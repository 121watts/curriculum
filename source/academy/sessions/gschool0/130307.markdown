---
layout: page
title: Thursday, March 7th
sidebar: true
---

## Daily Outline

* TrafficSpy Submissions
* Evaluating TrafficSpy
* TrafficSpy Code Reviews
* HTML & CSS
* OFA Event

## TrafficSpy Submissions

### Submissions

All project submissions are due here by 9:30AM:

https://github.com/gSchool/submissions/blob/master/projects/sales_engine.markdown

### Test Coverage

Your test suite should be generating a report in `coverage/index.html` with your test coverage percentage every time the suite is run.

### Code Cleanliness

Pull down the latest code from the project assignment to get access to the sanitation rake tasks:

{% terminal %}
$ git remote add upstream git://github.com/gSchool/traffic_spy.git
$ git pull upstream master
{% endterminal %}

Hopefully there will be no merge conflicts. Then:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:lines
$ bundle exec rake sanitation:methods
{% endterminal %}

Any reported issues will affect your style scoring -- fix them up if you have time!

## Evaluating TrafficSpy

You will peer assess *two different SalesEngine projects* using the instructions below.

### Evaluation Assignments



### Starting the Eval

Open http://eval.jumpstartlab.com and begin the evaluation for your first peer pair.

### Setup Folders and Code

* Create a folder on your system named `traffic_spy_evaluation_1`
* `cd` into that directory
* Clone the project you're evaluating by substituting the correct URL into this instruction:

{% terminal %}
$ git clone git://github.com/username/traffic_spy.git
{% endterminal %}

* Clone the data generator within the same `traffic_spy_evaluation_1` folder:

{% terminal %}
$ git clone git://github.com/gSchool/traffic_spy-generator.git
{% endterminal %}

### Start Their Server

Go into their project directory and start up the app:

{% terminal %}
$ cd traffic_spy
$ bundle
$ bundle exec shotgun
{% endterminal %}

### Correctness - Setup & Run the Data Generator

Go into the generator directory, install dependencies, and run the generator:

{% terminal %}
$ cd traffic_spy-generator
$ bundle
$ cucumber
{% endterminal %}

Observe the output from Cucumber for any errors. Then record your findings in the evaluation form.

```plain
Correctness
4: All provided tests pass without an error or crash
3: One test failed or crashed
2: Two or three tests failed or crashed
1: More than three tests failed or crashed
0: Program will not run
```

### Effort - Evaluating Extensions

There are no automated tests for extensions. If the project has extensions, look to their `README` for instructions or dig into their test suite.

Use this data in combination with results from the base expectations tests (above) to determine and record the Effort score.

```plain
Effort
5: Program fulfills all Base Expectations with Campaigns, JSON API, and (Dynamic Display or Authenticated Data)
4: Program fulfills all Base Expectations with Campaigns
3: Program fulfills all Base Expectations
2: Program fulfills Base Expectations except for one or two features.
1: Program fulfills some Base Expectations, but more than two features are broken.
0: Program does not fulfill any of the Base Expectations
```

### Testing - Evaluate Test Coverage

Next you'll need to check out their test coverage from within their project:

{% terminal %}
$ cd ../traffic_spy
$ bundle
$ bundle exec rspec spec
$ open coverage/index.html
{% endterminal %}

Use the code coverage percentage to determine and record the score.

```plain
Testing
4: Testing suite covers >95% of application code
3: Testing suite covers 90-94% of application code
2: Testing suite covers 80-89% of application code
1: Testing suite covers 50-89% of application code
0: Testing suite covers <50% of application code
```

### Style - Evaluate Code Style

Use the automated rake tasks to evaluate the code style from within the project directory:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:all
{% endterminal %}

Count the number of style infractions and determine/record the score.

```plain
Style
4: Source code consistently uses strong code style including lines under 80 characters, methods under 10 lines of code, and correct indentation.
3: Source code uses good code style, but breaks the above criteria in three or fewer spots
2: Source code uses mixed style, with three to six style breaks
1: Source code is generally messy with six to twelve issues
0: Source code is unacceptable, containing more than twelve style issues
```

### Peer Code Review

Spend the remainder of the evaluation period reading the code you're evaluating. Look for techniques that are particularly interesting. Some areas of focus:

* How did they do with using RSpec syntax? Are the tests nice and small, or giant blocks with lots of expectations? Did they make appropriate use of available techniques like `let`, `before(:each)`, etc?
* Check out their server (probably `lib/traffic_spy/server.rb`). Does it have a consistent level of abstraction? Are the responding blocks for each route small and maintainable, or a ton of lines?
* Were they able to break functionality into small objects, or are there just a few large objects doing everything?

Record these notes and thoughts in the **Highlights** and **Lowlights** sections of the evals.

### Submit the Evaluation

Review all your scores for accuracy, add any overall comments you have, and submit the eval.

### Rinse and Repeat

Each of you will evaluate two projects and be evaluated twice.
